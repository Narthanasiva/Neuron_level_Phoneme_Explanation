{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMpskBVgKjiurLsGtbZNsqI"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"0PTPbs_wdLpV","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1747122853882,"user_tz":-330,"elapsed":27256,"user":{"displayName":"Narthana Sivalingam","userId":"13596974493535450845"}},"outputId":"481fa747-6990-4ba5-f112-93c451875bbb"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"markdown","source":["# **Dataset Creation**"],"metadata":{"id":"DbUJ1Cgfo8jU"}},{"cell_type":"markdown","source":["## **1. Importing necessary libraries**"],"metadata":{"id":"FEAXA32Zqdez"}},{"cell_type":"code","source":["# --------------------------------------------------\n","# 0.  imports & folders\n","# --------------------------------------------------\n","import os, warnings, itertools, pickle\n","from pathlib import Path\n","from typing import List, Tuple, Dict, DefaultDict\n","from collections import defaultdict\n","import numpy as np\n","import pandas as pd\n","from tqdm import tqdm\n","import torch, torchaudio\n","from transformers import AutoFeatureExtractor, AutoModel\n","from sklearn.neural_network import MLPClassifier\n","from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n","import joblib"],"metadata":{"id":"FIHyNAOfpJSR"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## **2. Paths and variables**"],"metadata":{"id":"GJhbdwqMqiPk"}},{"cell_type":"code","source":["# --------------------------------------------------\n","# 1.  constants\n","# --------------------------------------------------\n","ROOT          = Path(\"/content/drive/MyDrive/00_RESEARCH_MSC_00/01_Phonetic_Identification/01_TIMIT_raw_dataset_whole\")   #Raw Data from TIMIT dataset(Whole data in one folder)\n","SAMPLE_RATE   = 16_000\n","FRAME_LEN     = 0.025\n","FRAME_SHIFT   = 0.020\n","MODEL_NAME    = \"microsoft/wavlm-base\"\n","DEVICE        = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","DATA_DIR   = Path(\"/content/drive/MyDrive/00_RESEARCH_MSC_00/03_Phonetic_Identification/layer_datasets\"); DATA_DIR.mkdir(exist_ok=True)\n"],"metadata":{"id":"XnPYVIIWpQeb"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## **3. Assigning phoneme Labels to each frames**"],"metadata":{"id":"cN3V_mGrrIvw"}},{"cell_type":"code","source":["# ------------------------------------------------------\n","# 2.  PHONEME → FEATURE TABLE\n","#     Build once using phonemes from the TIMIT phoncode list\n","#     Each phoneme is mapped to binary features: voiced, fricative, nasal\n","# ------------------------------------------------------\n","\n","# Set of all phonemes that are voiced\n","# Includes voiced consonants and all vowels (which are inherently voiced)\n","_voiced = {\n","    \"b\", \"d\", \"g\", \"dx\", \"jh\", \"z\", \"zh\", \"v\", \"dh\",                 # voiced obstruents\n","    \"m\", \"n\", \"ng\", \"em\", \"en\", \"eng\", \"nx\",                        # nasal consonants (voiced)\n","    \"l\", \"r\", \"w\", \"y\", \"hh\", \"hv\", \"el\",                           # approximants, liquids, glides\n","    # All vowel phonemes (always voiced)\n","    \"iy\", \"ih\", \"eh\", \"ey\", \"ae\", \"aa\", \"aw\", \"ay\", \"ah\", \"ao\",\n","    \"oy\", \"ow\", \"uh\", \"uw\", \"ux\", \"er\", \"ax\", \"ix\", \"axr\", \"ax-h\",\n","}\n","\n","# Set of fricative phonemes — produced with turbulent airflow\n","_fric = {\"s\", \"sh\", \"z\", \"zh\", \"f\", \"th\", \"v\", \"dh\", \"hh\", \"hv\"}\n","\n","# Set of nasal phonemes — air flows through the nasal cavity\n","_nasal = {\"m\", \"n\", \"ng\", \"em\", \"en\", \"eng\", \"nx\"}\n","\n","# Function to build a dictionary that maps each phoneme to its phonetic feature labels\n","def build_feature_dict() -> Dict[str, Dict[str, int]]:\n","    # Set of all TIMIT phonemes, including:\n","    #   - Plosives: p, t, k, b, d, g\n","    #   - Affricates, fricatives, nasals, vowels\n","    #   - Closure markers: bcl, dcl, etc.\n","    #   - Special tokens: pau (pause), epi (epenthetic stop), h# (start/end silence)\n","    phones = {\n","        \"b\", \"d\", \"g\", \"p\", \"t\", \"k\", \"bcl\", \"dcl\", \"gcl\", \"pcl\", \"tcl\", \"kcl\",\n","        \"dx\", \"q\", \"jh\", \"ch\", \"s\", \"sh\", \"z\", \"zh\", \"f\", \"th\", \"v\", \"dh\",\n","        \"m\", \"n\", \"ng\", \"em\", \"en\", \"eng\", \"nx\",\n","        \"l\", \"r\", \"w\", \"y\", \"hh\", \"hv\", \"el\",\n","        \"iy\", \"ih\", \"eh\", \"ey\", \"ae\", \"aa\", \"aw\", \"ay\", \"ah\", \"ao\", \"oy\", \"ow\",\n","        \"uh\", \"uw\", \"ux\", \"er\", \"ax\", \"ix\", \"axr\", \"ax-h\",\n","        \"pau\", \"epi\", \"h#\"\n","    }\n","\n","    # Initialize the feature mapping dictionary\n","    table = {}\n","\n","    # Loop over each phoneme and assign binary feature values:\n","    #   1 if the phoneme belongs to the corresponding feature set, else 0\n","    for ph in phones:\n","        table[ph] = {\n","            \"voiced\":    int(ph in _voiced),     # 1 if voiced, else 0\n","            \"fricative\": int(ph in _fric),       # 1 if fricative, else 0\n","            \"nasal\":     int(ph in _nasal),      # 1 if nasal, else 0\n","        }\n","\n","    # Return the complete phoneme-feature mapping\n","    return table\n","\n","# Build the feature dictionary once — can be reused in later processing\n","FEAT = build_feature_dict()\n","\n","# List of feature names used in the dictionary\n","FEATURES = [\"voiced\", \"fricative\", \"nasal\"]\n"],"metadata":{"id":"Ms_lDMTaq3hs"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Sample data format after the above processing\n","{\n","\n","  \"s\":   {\"voiced\": 0, \"fricative\": 1, \"nasal\": 0},\n","\n","  \"m\":   {\"voiced\": 1, \"fricative\": 0, \"nasal\": 1},\n","\n","  \"z\":   {\"voiced\": 1, \"fricative\": 1, \"nasal\": 0},\n","\n","  \"ng\":  {\"voiced\": 1, \"fricative\": 0, \"nasal\": 1},\n","\n","  \"iy\":  {\"voiced\": 1, \"fricative\": 0, \"nasal\": 0},\n","\n","  \"pau\": {\"voiced\": 0, \"fricative\": 0, \"nasal\": 0},\n","  ...\n","}"],"metadata":{"id":"aNsu3B9A_6NG"}},{"cell_type":"markdown","source":["## **4. Loading the WAVLM model**"],"metadata":{"id":"6tC5IPP7rYtc"}},{"cell_type":"code","source":["# --------------------------------------------------\n","# 3.  WavLM  (output_hidden_states=True!)\n","# --------------------------------------------------\n","print(\"Loading WavLM‑base …\")\n","feature_extractor = AutoFeatureExtractor.from_pretrained(MODEL_NAME)\n","model = AutoModel.from_pretrained(MODEL_NAME,\n","                                  output_hidden_states=True).to(DEVICE).eval()\n","EMB_DIM = model.config.hidden_size      # 768"],"metadata":{"id":"6kY9TtiArXhX"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## **5. Helper functions**"],"metadata":{"id":"6c_1k10yrgdq"}},{"cell_type":"code","source":["# --------------------------------------------------\n","# 4. Helper Functions (With Simple Explanations)\n","# --------------------------------------------------\n","\n","from typing import List, Tuple\n","import torch\n","from pathlib import Path\n","\n","def read_phn(pfile: Path) -> List[Tuple[float, float, str]]:\n","    \"\"\"\n","    This function reads a TIMIT .PHN file and returns a list of:\n","    (start_time, end_time, phoneme)\n","\n","    - Each line in the .PHN file gives one phoneme and its time in samples.\n","    - We convert start and end times from sample count to seconds.\n","    \"\"\"\n","    rows: List[Tuple[float, float, str]] = []\n","\n","    # Open the file\n","    with open(pfile) as fh:\n","        for line in fh:\n","            # Split the line into start, end, and phoneme label\n","            s, e, ph = line.strip().split()\n","            # Convert sample indexes to seconds and store in list\n","            rows.append((int(s) / SAMPLE_RATE, int(e) / SAMPLE_RATE, ph))\n","\n","    return rows\n","\n","\n","\n","\n","def frame_bounds(n: int) -> List[Tuple[float, float]]:\n","    \"\"\"\n","    This function returns the time window (start_time, end_time) for each frame.\n","\n","    - n: number of frames\n","    - Each frame starts at i * FRAME_SHIFT and lasts for FRAME_LEN seconds.\n","    \"\"\"\n","    return [(i * FRAME_SHIFT, i * FRAME_SHIFT + FRAME_LEN) for i in range(n)]\n","\n","\n","\n","\n","def phone_avg(\n","    layer_embs: torch.Tensor,\n","    times: List[Tuple[float, float]],\n","    phone_ints: List[Tuple[float, float, str]]\n",") -> Tuple[str, torch.Tensor]:\n","    \"\"\"\n","    This function averages the frame embeddings for each phoneme.\n","\n","    - It goes through each phoneme time range (ps, pe).\n","    - Then it selects only those frames that are completely inside that phoneme's time range.\n","    - It calculates the mean of those frames' embeddings and returns (phoneme, mean_embedding).\n","    \"\"\"\n","    for ps, pe, ph in phone_ints:\n","        # Find which frames are inside this phoneme time window\n","        mask = torch.tensor([(t0 >= ps) and (t1 <= pe) for t0, t1 in times])\n","\n","        # If any frame falls inside, compute mean\n","        if mask.any():\n","            # Average the embeddings of selected frames\n","            yield ph, layer_embs[mask].mean(dim=0)\n","\n","\n","\n","\n","def phone_frames(\n","    layer_embs: torch.Tensor,\n","    times: List[Tuple[float, float]],\n","    phone_ints: List[Tuple[float, float, str]]\n",") -> Tuple[str, torch.Tensor]:\n","    \"\"\"\n","    This function assigns each frame to the phoneme it belongs to.\n","\n","    - For every frame (with time window t0, t1), find the phoneme it falls into.\n","    - It returns (phoneme, frame_embedding) for each frame.\n","    \"\"\"\n","    phone_idx = 0  # Start from the first phoneme\n","\n","    # Go through each frame\n","    for idx, (t0, t1) in enumerate(times):\n","\n","        # Move to the correct phoneme (if frame is past current phoneme's end)\n","        while phone_idx < len(phone_ints) - 1 and t0 >= phone_ints[phone_idx][1]:\n","            phone_idx += 1\n","\n","        # Return the current phoneme and its corresponding frame's embedding\n","        yield phone_ints[phone_idx][2], layer_embs[idx]\n"],"metadata":{"id":"yzvH3sdcrfNM"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## **6. Build Dataset for every Layer**"],"metadata":{"id":"ecjz-mC0rudo"}},{"cell_type":"code","source":["# --------------------------------------------------\n","# 5.  pass 1 – build datasets for every layer\n","# --------------------------------------------------\n","train_rows  : DefaultDict[int, list] = defaultdict(list)   # key = layer\n","test_rows   : DefaultDict[int, list] = defaultdict(list)\n","\n","wav_files = sorted(ROOT.glob(\"*.wav\"))\n","\n","for wav in tqdm(wav_files, desc=\"extracting embeddings\"):\n","    stem = wav.stem\n","    phn  = ROOT / f\"{stem}.phn\"\n","    if not phn.exists():\n","        warnings.warn(f\"{stem}: no .phn, skipping\"); continue\n","\n","    # raw audio\n","    speech, sr = torchaudio.load(wav)\n","    if sr != SAMPLE_RATE:\n","        speech = torchaudio.functional.resample(speech, sr, SAMPLE_RATE)\n","    speech = speech.squeeze()\n","\n","    inp = feature_extractor(speech, sampling_rate=SAMPLE_RATE,\n","                            return_tensors=\"pt\", padding=True)\n","    with torch.no_grad():\n","        hstates = model(**{k:v.to(DEVICE) for k,v in inp.items()}).hidden_states\n","        # hstates: tuple length 13  (0 = CNN, 12 = top)\n","        hstates = [h.cpu().numpy()[0] for h in hstates]   # each  (T,768)\n","\n","    times  = frame_bounds(len(hstates[0]))\n","    phones = read_phn(phn)\n","\n","    is_train = np.random.rand() < 0.95\n","\n","    for L in range(13):\n","        if is_train:\n","            for ph, vec in phone_avg(hstates[L], times, phones):\n","                train_rows[L].append(\n","                    {\"embedding\": vec, \"phone\": ph, **FEAT.get(ph)})\n","        else:\n","            for ph, vec in phone_frames(hstates[L], times, phones):\n","                test_rows[L].append(\n","                    {\"embedding\": vec, \"phone\": ph, **FEAT.get(ph)})"],"metadata":{"id":"_ySmL6YNrrtH"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## **7. Save every layer Dataframe**"],"metadata":{"id":"UWZFVjOtteB8"}},{"cell_type":"code","source":["# --------------------------------------------------\n","# Save datasets (for each layer) as .pkl files\n","# --------------------------------------------------\n","\n","# Loop through all 13 WavLM layers (0 to 12)\n","for L in range(13):\n","    # Convert list of training rows for this layer into a pandas DataFrame\n","    train_df = pd.DataFrame(train_rows[L])\n","\n","    # Convert list of testing rows for this layer into a DataFrame\n","    test_df = pd.DataFrame(test_rows[L])\n","\n","    # Save training DataFrame to a .pkl file\n","    train_df.to_pickle(DATA_DIR / f\"layer_{L}_train.pkl\")\n","\n","    # Save testing DataFrame to a .pkl file\n","    test_df.to_pickle(DATA_DIR / f\"layer_{L}_test.pkl\")\n","\n","# Print message to confirm all datasets are saved\n","print(\"✅ All layer datasets have been saved in:\", DATA_DIR)\n"],"metadata":{"id":"akNHGfexo_e1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install ipynbname\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rWLvwjPe-MjZ","executionInfo":{"status":"ok","timestamp":1747849898904,"user_tz":-330,"elapsed":6298,"user":{"displayName":"Narthana Sivalingam","userId":"13596974493535450845"}},"outputId":"b29c2238-730d-4c46-f317-119b757fe73e"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting ipynbname\n","  Downloading ipynbname-2024.1.0.0-py3-none-any.whl.metadata (1.9 kB)\n","Requirement already satisfied: ipykernel in /usr/local/lib/python3.11/dist-packages (from ipynbname) (6.17.1)\n","Requirement already satisfied: debugpy>=1.0 in /usr/local/lib/python3.11/dist-packages (from ipykernel->ipynbname) (1.8.0)\n","Requirement already satisfied: ipython>=7.23.1 in /usr/local/lib/python3.11/dist-packages (from ipykernel->ipynbname) (7.34.0)\n","Requirement already satisfied: jupyter-client>=6.1.12 in /usr/local/lib/python3.11/dist-packages (from ipykernel->ipynbname) (6.1.12)\n","Requirement already satisfied: matplotlib-inline>=0.1 in /usr/local/lib/python3.11/dist-packages (from ipykernel->ipynbname) (0.1.7)\n","Requirement already satisfied: nest-asyncio in /usr/local/lib/python3.11/dist-packages (from ipykernel->ipynbname) (1.6.0)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from ipykernel->ipynbname) (24.2)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from ipykernel->ipynbname) (5.9.5)\n","Requirement already satisfied: pyzmq>=17 in /usr/local/lib/python3.11/dist-packages (from ipykernel->ipynbname) (24.0.1)\n","Requirement already satisfied: tornado>=6.1 in /usr/local/lib/python3.11/dist-packages (from ipykernel->ipynbname) (6.4.2)\n","Requirement already satisfied: traitlets>=5.1.0 in /usr/local/lib/python3.11/dist-packages (from ipykernel->ipynbname) (5.7.1)\n","Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.11/dist-packages (from ipython>=7.23.1->ipykernel->ipynbname) (75.2.0)\n","Collecting jedi>=0.16 (from ipython>=7.23.1->ipykernel->ipynbname)\n","  Downloading jedi-0.19.2-py2.py3-none-any.whl.metadata (22 kB)\n","Requirement already satisfied: decorator in /usr/local/lib/python3.11/dist-packages (from ipython>=7.23.1->ipykernel->ipynbname) (4.4.2)\n","Requirement already satisfied: pickleshare in /usr/local/lib/python3.11/dist-packages (from ipython>=7.23.1->ipykernel->ipynbname) (0.7.5)\n","Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from ipython>=7.23.1->ipykernel->ipynbname) (3.0.51)\n","Requirement already satisfied: pygments in /usr/local/lib/python3.11/dist-packages (from ipython>=7.23.1->ipykernel->ipynbname) (2.19.1)\n","Requirement already satisfied: backcall in /usr/local/lib/python3.11/dist-packages (from ipython>=7.23.1->ipykernel->ipynbname) (0.2.0)\n","Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.11/dist-packages (from ipython>=7.23.1->ipykernel->ipynbname) (4.9.0)\n","Requirement already satisfied: jupyter-core>=4.6.0 in /usr/local/lib/python3.11/dist-packages (from jupyter-client>=6.1.12->ipykernel->ipynbname) (5.7.2)\n","Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.11/dist-packages (from jupyter-client>=6.1.12->ipykernel->ipynbname) (2.9.0.post0)\n","Requirement already satisfied: parso<0.9.0,>=0.8.4 in /usr/local/lib/python3.11/dist-packages (from jedi>=0.16->ipython>=7.23.1->ipykernel->ipynbname) (0.8.4)\n","Requirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.11/dist-packages (from jupyter-core>=4.6.0->jupyter-client>=6.1.12->ipykernel->ipynbname) (4.3.8)\n","Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.11/dist-packages (from pexpect>4.3->ipython>=7.23.1->ipykernel->ipynbname) (0.7.0)\n","Requirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=7.23.1->ipykernel->ipynbname) (0.2.13)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.1->jupyter-client>=6.1.12->ipykernel->ipynbname) (1.17.0)\n","Downloading ipynbname-2024.1.0.0-py3-none-any.whl (4.3 kB)\n","Downloading jedi-0.19.2-py2.py3-none-any.whl (1.6 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m16.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: jedi, ipynbname\n","Successfully installed ipynbname-2024.1.0.0 jedi-0.19.2\n"]}]}]}